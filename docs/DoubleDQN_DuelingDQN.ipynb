{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double-DQN, Priority Experience Replay (PER) and Dueling-DQN \n",
    "\n",
    "Reference:\n",
    "+ Dual DQN paper: https://arxiv.org/abs/1509.06461\n",
    "+ Julien Blog: https://julien-vitay.net/deeprl/Valuebased.html#sec:double-dqn \n",
    "+ Efficient implement of Priority Experience Replay: https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Fixing problems of DQN:\n",
    "### 1. Fixing Over-Estimate problem with Dual-DQN:\n",
    "Recall the steps to update the target network in the original [DQN](Q-Learning.ipynb), \n",
    "+ S1: Randomly sample a batch of data from the buffer: $\\{(s_i,a_i,r_i,s_{i+1})\\}$.\n",
    "+ S2: Compute the Target value $Q_{target}= r_i + \\gamma Q_{\\phi'}(s_{i+1},a')$, where $a'=\\argmax Q_{\\phi'}(s_{i+1},a')$.\n",
    "+ S3: Compute the loss: $L=Mean(Q_{target} - Q_\\phi(s_i,a_i))^2$, and perform SGD to update $\\phi.\n",
    "\n",
    "**Problem:** Note that in step S2, we use the same target network $Q_{\\phi'}$ to (i) select the action $a'=\\argmax Q_{\\phi'}(s_{i+1},a')$ and (ii) update the target: $Q_{target}= r_i + \\gamma Q_{\\phi'}(s_{i+1},a')$. What 's the problem with it?\n",
    "+ Chicken-Egg problem of Boostraping: We use an **estimated function $Q_{\\phi'}$**  to **estimate the target** , and use this target to **update the estimation $Q_{\\phi}$ again**. \n",
    "+ Especially at the beginning of learning when Q-values are far from being correct, because we select an action $a'$ that maximizes the estimated $Q_{\\phi'}$, the target $Q_{target}$ is then frequently over-estimating (higher than the true return). This leads to the learned Q-value $Q_{\\phi}$ will also become over-estimated, and the error will then propagate to all previous actions on the long-term.  \n",
    "+ Example of Over-Estimate: At some state, all actions are equally good. However, because the estimated Q-value is noisy, we select an action based on Max Q-value. This leads to over-estimate the value of that action.\n",
    "   \n",
    "**Solution:** Dual DQN uses two independent value networks: (i) one to find the greedy action $a'$, (ii) one to estimate the Q-value itself. So, even if the first network chooses an over-estimated action as the greedy action, the other might provide a less over-estimated value for it, solving the problem.\n",
    "\n",
    "**Implementation**  Applying Dual-DQN is very simple, since we already have two networks. In Step S2, just break it into two steps:\n",
    "  + Greedy Action Selection by the main Q-network: change from DQN $a'=\\argmax Q_{\\phi'}(s_{i+1},a')$ to Dual-DQN: $a'=\\argmax Q_{\\phi}(s_{i+1},a')$ \n",
    "  + Estimate Q-Target in the same way: $Q_{target}= r_i + \\gamma Q_{\\phi'}(s_{i+1},a') $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fixing Slow Convergence with Priority Experience Replay:\n",
    "+ **Problem:** Another drawback of DQN is that a data batch is uniformly sampled. However, during the learning process, informative transitions are very sparse relatively to the rest of the data. Thus, uniform sampling is not very effective.  \n",
    "+ **Solution:** Instead, if we can rank the data point by its level of usefulness and sample them based on its priority, we can speed up the learning. One good criterion to rank the data is using the Temporal-Difference error (TD-error), which express the supprising results we get:\n",
    "  $$ \\delta_i = Q_{target}(r_i,s_{i+1},a') - Q_{\\phi}(s_i,a_i)$$ \n",
    "  That is, we prefer transitions that do not fit well to our current estimate of the Q function, because these are the transitions that we can learn most from. This reflects a simple intuition from our real world - if we encounter a situation that really differs from our expectation, we think about it over and over and try to solve it until it makes sense (e.g. until our model can fit it). Therefore, we update the priority of the transitions after updating the Q-networks.\n",
    "+ **Implementation:** However, low $\\delta$ transitions might become relevant again after enough training, as the $Q_\\phi(s,a)$ change, so prioritized replay has a softmax function over the TD error to ensure “exploration” of memorized transitions.\n",
    "  $$ p_i = \\frac{e^{-\\alpha \\delta_i} }{\\sum e^{-\\alpha \\delta_i}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fixing high variance of Q-Function with Dueling-DQN:\n",
    "+ **Problem:** Q-function has high variance (noisy). This is because that Q-value function is estimated based on previous estimates, which itself evolves during learning (non-stationarity of the targets) and can drastically change during exploration (stochastic policies).\n",
    "+ **Solution:** Recall from [ProblemFormulation_Notation](ProblemFormulation_Notation.ipynb), \n",
    "> *Advantage function*: describe how much better to take a specific action $a$ in state $s$, over randomly selecting an action according to $\\pi(\\cdot|s)$: \n",
    ">  $$ A^{\\pi}(s,a)= Q^\\pi(s,a) - V^{\\pi}(s) $$\n",
    ">  Intuitively, the Action-Value Q-function depends on two factors:\n",
    ">  + The value of the underlying state $s$: In some state, all actions are bad. You lose no matter action you take.\n",
    ">  + The action you act on the state: Some action may be better than the others. \n",
    "\n",
    "Advantage value tells you how much better an action compared to an expected value when taking random action. Because we subtract the value of the state (which is the mean value of all action), the advantage has less variance than the Q-value, and that is much more stable over time for Optimization.\n",
    "\n",
    "Therefore, we can decompose the Q-function from the value function, and use the advantage function to describe how much better to take an action in a state.\n",
    "$$  Q^\\pi(s,a) = V^{\\pi}(s) + A^{\\pi}(s,a)$$\n",
    "In Implementation, we attach two heads to the backbone network, one to estimate $V$ and another to estimate $A$, as illustrated in Fig: \n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-03_at_3.24.01_PM.png\"\n",
    "     alt=\"RL taxanomy\"\n",
    "     width=\"400\"\n",
    "     style=\"float: left; margin-right: 20px;\" />\n",
    "\n",
    "\n",
    "+ **Implementation** In practice, we use the following formula for better stability:\n",
    "  $$ Q_{\\phi,\\alpha,\\beta}(s,a) = V_{\\alpha}(f) + \\left(A_{\\beta}(f,a) - \\frac{1}{N}\\sum_a A_{\\beta}(f,a) \\right), \\quad f = B_\\phi(s)$$\n",
    "  where $B_\\phi$ is the backbone network to extract feature $f$, $V_{\\alpha}$ is the value network, $A_\\beta$ is the advantage network, and $N$ is the number of action."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
