{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation and Notation \n",
    "\n",
    "Reference:\n",
    "+ https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology\n",
    "+ https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Notations:\n",
    "### 1. Policy:\n",
    "A policy is a rule used by an agent to decide what actions to take:\n",
    "+ It can be deterministic, denoted as: $a_t=\\mu(s_t)$\n",
    "+ It may be stochastic, often denoted as: $a_t \\sim \\pi(\\cdot | s_t)$\n",
    "\n",
    "In Deep RL, a policy is parameterized by an Neural Network parameters $\\theta$, hence we write the policy as:\n",
    "+ deterministic: $a_t=\\mu_{\\theta}(s_t)$\n",
    "+ stochastic: $a_t \\sim \\pi_{\\theta}(\\cdot | s_t)$. \n",
    "  \n",
    "### 2. Trajectory: \n",
    "A trajectory $\\tau$ is a sequence of states $s_t$ and actions $a_t$ in the world: $\\tau=(s_0,a_0,s_1,a_1,...)$:\n",
    "+ state $s_0$ is random assigned when starting env: $s_0 \\sim \\rho_0$.\n",
    "+ state $s_t$ depends only on the current state and action. Assuming that it is stochastic with *transition probability*: $s_t \\sim P(\\cdot|s_t,a_t)$. \n",
    "+ assuming action $a_t$ is ruled by the stochastic policy: $a_t \\sim \\pi_\\theta(\\cdot|s_t)$.\n",
    "  \n",
    "Then, the probability of T-step trajectory is:\n",
    "    $$P(\\tau|\\pi) = \\rho_0(s_0) \\prod_{t=0}^{T-1}P(s_{t+1}|s_t,a_t)\\pi_\\theta(a_t|s_t)$$\n",
    "\n",
    "### 3. Reward-Return and the Objective of RL:\n",
    "+ *reward* is received after the agent performs an action $a_t$ at the state $s_t$: $r_t=R(s_t,a_t)$.\n",
    "+ *return of a trajectory* $\\tau$ is the sum of collected rewards $r_t$ weighted by a discount factor $\\gamma$: $R(\\tau) = \\sum_{t=0}^T \\gamma^t r_t$.\n",
    "+ *the goal of RL* is to select an optimal policy $\\pi^*$ which **maximizes expected return $J(\\pi)$** when agent acts according to it:\n",
    "    $$\\pi^*= \\argmax_\\pi J(\\pi) \\quad \\text{where} \\quad J(\\pi)= \\mathbb{E}_{\\tau \\sim \\pi} [R(\\tau)] = \\int_\\tau P(\\tau|\\pi) R(\\tau) $$\n",
    "It is often intimidate when seeing $\\mathbb{E}_{\\tau \\sim \\pi}$ and $\\int_\\tau$ in the equation. However, they are one thing but writen in different form for different use case:\n",
    "   + Use $\\mathbb{E}_{\\tau \\sim \\pi}$ when we want to emphasize the intuitive meaning, that is the expected value when sampling $\\tau$ according to a distribution $\\pi$.\n",
    "   + Use $\\int_\\tau P(\\tau|\\pi)$ when we want to use some magic math to formulate and solve the equation. \n",
    "   + Use $\\frac{1}{T}\\sum_{t=0}^T R(\\tau_t)$ when we actually implement it by taking the average.   \n",
    "### 4. Value function: \n",
    "If we start from a state $s$ or state-action pair $(s,a)$, we denote the **the value** of $s$ or $(s,a)$ as the expected returns when we strictly follow the policy $\\pi$ forever after:\n",
    "+ *Value function*: $V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim \\pi} [R(\\tau) | s_0=s]$ \n",
    "+ *Action-Value function*: $Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau \\sim \\pi} [R(\\tau) | s_0=s, a_0=a]$ \n",
    "+ Connection between $V^{\\pi}(s)$ and $Q^{\\pi}(s,a)$:\n",
    "    $$ V^{\\pi}(s) = \\mathbb{E}_{\\tau \\sim \\pi} [R(\\tau) | s_0=s] = \\int_a \\pi(a|s_0) \\mathbb{E}_{\\tau \\sim \\pi} [R(\\tau) | s_0=s, a_0=a] = \\mathbb{E}_{a \\sim \\pi}[Q^{\\pi}(s,a)]$$\n",
    "+ *Advantage function*: describe how much better to take a specific action $a$ in state $s$, over randomly selecting an action according to $\\pi(\\cdot|s)$: \n",
    "    $$ A^{\\pi}(s,a)= Q^\\pi(s,a) - V^{\\pi}(s) $$\n",
    "  Intuitively, the Action-Value Q-function depends on two factors:\n",
    "    + The value of the underlying state $s$: In some state, all actions are bad. You lose no matter action you take.\n",
    "    + The action you act on the state: Some action may be better than the others. \n",
    "    + Advantage value: tell you how much better an action compared to an expected value when taking random action.  \n",
    "+ If we follow the optimal policy at the state $s$, we call it as *Optimal Value function* $V^*(s)$.\n",
    "+ If we follow the optimal policy after taking an action $a$ at the state $s$, we call it as *Optimal Action-Value function* $Q^*(s,a)$. \n",
    "+ Their connection is: $ V^*(s)= \\max_a  Q^*(s,a)$.\n",
    "+ The optimal action: $a^*(s) = \\argmax_a Q^*(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Kind of RL Algorithms:\n",
    "RL Algorithms can be divided into:\n",
    "+ Model-Free\n",
    "+ Model-based methods. \n",
    "  \n",
    "Model-Free methods can then divided into two-main approaches:\n",
    "  + **On-Policy (or Policy Optimization)**: We learn the optimal policy $\\pi_\\theta(\\cdot|s)$ directly, and always act according to the policy. The procedure generally includes :\n",
    "    + Initialize random policy  $\\pi_\\theta(\\cdot|s)$.\n",
    "    + For every n-step in each episode, do:\n",
    "      + Follow the policy $\\pi_\\theta(a|s)$ to select action at each step $t$, and collect the rewards $r_t$.\n",
    "      + After n-steps, compute the return $R(\\tau)$, and the loss value $J(\\pi_\\theta)$. Update the policy by SGD: \n",
    "        $$\\theta \\leftarrow \\theta + \\nabla_\\theta J(\\pi_\\theta)$$\n",
    "  + **Off-Policy (or Q-learning)**: We learn the Action Value function $Q_\\phi(s,a)$, and select the optimal action that maximize $a^*(s) = \\argmax_a Q_{\\phi}(s,a)$. There is no direct policy in here, and the procedure generally includes:\n",
    "    + Initialize random Q-value function:  $Q_{\\phi}(s,a)$.  \n",
    "    + For every n-step in each episode, do:\n",
    "      + Select an action that maximize $Q(s,a)$, but with probability $\\epsilon$, select the action randomly. Add the data $(s_t,a_t,r_t,s_{t+1},done)$ to the data buffer.\n",
    "      + Do sampling data from buffer, and update the $Q_\\phi(s,a)$ network by SGD. \n",
    "        $$ \\phi \\leftarrow \\phi + \\nabla (Q_\\phi(s,a)-Q_T(s,a))^2 $$\n",
    "        where $Q_T(s,a)$ is the target of Q-Value, that we don't know and must use a boostraping technique to approximate it.  \n",
    "\n",
    "Therefore, the difference between On-Policy and Off-Policy is that:\n",
    "|  \t| On-Policy \t| Off Policy \t|\n",
    "|---\t|---\t|---\t|\n",
    "| Principle \t| - Strictly follow the policy in in n steps.  \t| - There is no explicit policy. Instead, we approximate the Q-value, and indirectly infer the optimal action. |\n",
    "| Update      |- Update the policy using the newest data collected in the last n-steps. |  - Update the Q-function using data randomly sampled from the buffer collected in all episodes, regardless the time order. \t|\n",
    "| Advantage \t| - Stable and more intuitive by directly optimize the policy. \t| - Data efficient, and update the network faster if it works. \t|\n",
    "| Disadvantage \t| - Slow convergence, and data insufficient \t| - Can be very unstable. There is no guarantee that if the Q-Value is optimized can lead to optimal policy.  $Q_T$ is generally a weak approximation.\t|\n",
    "\n",
    "  + **Actor-Critic** approaches combines the advantages of both On-Policy and Off-policy:\n",
    "    + It has an Actor Network $\\{\\theta\\}$ to approximate the Policy $\\pi_\\theta(a,s)$.\n",
    "    + and a Critic Network $\\{\\phi\\}$  to estimate the Q-value function $Q_{\\phi}(s,a)$ or V-value function $V_{\\phi}(s)$. \n",
    "    + The result is generally better than each individual one. In fact, most of the advanced algorithms employ the Actor-Critic structure, and only differs in the way of chosing action: using Policy (then it is On-Policy) or Q-function (then it is Off-Policy). In many cases, the boundary is not clear. \n",
    "  \n",
    "The follow diagram illustrates the taxanomy of the common RL algorithms\n",
    "\n",
    "<img src=\"https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg\"\n",
    "     alt=\"RL taxanomy\"\n",
    "     width=\"800\"\n",
    "     style=\"float: left; margin-right: 20px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "+ [Deep Q-Learning](Q-Learning.ipynb): to learn basic of Off-Policy.\n",
    "+ [Vanila Policy Gradient](Vanila_Policy_Optimization.ipynb): to learn basic of On-Policy \n",
    "+ [Config Usage and Write new Algorithm](Config_Usage.md): to learn how to use config file and write new algorithm."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5734d3a3e606dbf63d950d18f54ab035ee459033a33e8c50ee247c9aa7f517bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('chuong_RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
